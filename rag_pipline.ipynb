{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d2ec4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain-google-genai langchain langchain-community faiss-cpu pypdf\n",
    "!pip install langchain-classic\n",
    "!pip install -U langchain-huggingface\n",
    "!pip install google-generativeai \n",
    "!pip install langchain-community sentence-transformers\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "import getpass\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your API key:\")\n",
    "print(\"API key set succesfully\")\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_path = r\"C:\\Users\\LENOVO\\Downloads\\HealthCareSectorinindia-AnOverview.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "!pip install hf_xet\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunks,embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-2.5-flash\",\n",
    "    temperature=0.2\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "#building RAG Chain\n",
    "qa= ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "#context relevance score\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def context_relevance_score(query_embeddings,docs_embeddings):\n",
    "    similarities = cosine_similarity(query_embeddings,docs_embeddings)    \n",
    "    return round(float(similarities.mean()),3)\n",
    "\n",
    "#faithfulness score\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\",\"answer\"],\n",
    "    template=\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Rate how much the answer is supported by the context.\n",
    "Score between 0 and 1.\n",
    "Only return the number.\n",
    "\"\"\")\n",
    "def faithfulness_score(llm,context,answer):\n",
    "    response = llm.invoke(\n",
    "        PROMPT.format(context=context,answer=answer)\n",
    "    )\n",
    "    return round(float(response.content.strip()),3)\n",
    "\n",
    "\n",
    "def context_relevance_score(query, context):\n",
    "    q_words = set(query.lower().split())\n",
    "    c_words = set(context.lower().split())\n",
    "    if not q_words:\n",
    "        return 0.0\n",
    "    return round(len(q_words & c_words) / len(q_words), 3)\n",
    "\n",
    "def faithfulness_score(answer, context):\n",
    "    a_words = set(answer.lower().split())\n",
    "    c_words = set(context.lower().split())\n",
    "    if not a_words:\n",
    "        return 0.0\n",
    "    return round(len(a_words & c_words) / len(a_words), 3)\n",
    "\n",
    "def completeness_score(answer):\n",
    "    return round(min(len(answer.split()) / 50, 1.0), 3)\n",
    "\n",
    "\n",
    "#creating a trace logger\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "LOG_FILE = \"trace_logs.csv\"\n",
    "\n",
    "def log_trace(data: dict):\n",
    "    \n",
    "    #timestamp\n",
    "    data[\"timestamp\"] = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    file_exists = os.path.isfile(LOG_FILE)\n",
    "\n",
    "    with open(LOG_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=data.keys())\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow(data)\n",
    "\n",
    "\n",
    "while True:\n",
    "    query = input(\"Ask: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    result = qa.invoke({\"question\": query})\n",
    "    answer = result[\"answer\"]\n",
    "    print(\"\\nAnswer:\\n\", answer)\n",
    "\n",
    "    docs = retriever.invoke(query)\n",
    "    retrieved_text = \" \".join([d.page_content[:300] for d in docs])\n",
    "\n",
    "    cr_score = context_relevance_score(query, retrieved_text)\n",
    "    f_score = faithfulness_score(answer, retrieved_text)\n",
    "    comp_score = completeness_score(answer)\n",
    "\n",
    "    \n",
    "\n",
    "log_trace({\n",
    "        \"query\": query,\n",
    "        \"retrieved_context\": retrieved_text,\n",
    "        \"answer\": answer,\n",
    "        \"context_relevance_score\":cr_score,\n",
    "        \"faithfulness_score\":f_score,\n",
    "        \"completeness_score\":comp_score,\n",
    "        \"num_docs\": len(docs),\n",
    "        \"model\": \"models/gemini-2.5-flash\"\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
