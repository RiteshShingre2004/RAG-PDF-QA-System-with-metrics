# ğŸ“„ RAG PDF Question Answering System with Evaluation Metrics

An **industry-grade Retrieval-Augmented Generation (RAG)** system that allows users to ask natural language questions over PDF documents and receive **context-aware, grounded answers**, along with **automatic evaluation metrics**.

---

ğŸš€ Project Overview

This project implements a **PDF-based Question Answering system** using a **Retrieval-Augmented Generation (RAG) pipeline**.  
Relevant document chunks are retrieved from PDFs and passed to a **Gemini LLM** to generate accurate answers while tracking evaluation metrics.

This project demonstrates **real-world RAG system design, observability, and evaluation**, suitable for enterprise AI applications.

---

ğŸ§  Key Features

- ğŸ“‚ PDF document ingestion
- ğŸ” Semantic document retrieval (Top-K)
- ğŸ¤– LLM-powered answer generation (Gemini)
- ğŸ“Š Automatic evaluation metrics
- ğŸ§¾ Query & response trace logging
- ğŸ§± Modular, extensible pipeline

---

---

## ğŸ“Œ Technologies Used

- Python
- Jupyter Notebook
- Google Gemini LLM
- Vector-based retrieval
- NumPy, Pandas
- CSV logging
- PDF parsing

---

ğŸ“Š Evaluation Metrics

| Metric | Description |
|------|------------|
| Context Relevance | Measures relevance of retrieved context to query |
| Faithfulness | Ensures answer is grounded in retrieved context |
| Completeness | Checks if the answer sufficiently covers the query |

These metrics help identify hallucinations and improve answer reliability.

---

---

âš™ï¸ Installation & Setup

 Clone the repository
```bash
git clone https://github.com/RiteshShingre2004/RAG-PDF-QA-System-with-metrics.git
cd RAG-PDF-QA-System-with-metrics